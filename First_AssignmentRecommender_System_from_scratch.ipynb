{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANASS19/AI/blob/main/First_AssignmentRecommender_System_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 2: Recommender System from Scratch"
      ],
      "metadata": {
        "id": "yVlwEEkWlXEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giorgio Lazzarinetti - My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "giorgiolazzarinetti@gmail.com g.lazzarinetti@campus.unimib.it"
      ],
      "metadata": {
        "id": "aUSaHRazlauW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Outline\n",
        "\n",
        "- **Introduction to Recommender System**\n",
        "- **Movielens Dataset**\n",
        "- **Generalized Matrix Factorization Model**\n",
        "- **Neural Collaborative Filtering**\n",
        "- **LAB CHALLENGE 1: Neural Matrix Factorization**"
      ],
      "metadata": {
        "id": "Kg__F8YqsqGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "The architecture of the deep model, the evaluation strategy and the metrics used are taken from the paper: [\"Neural Collaborative Filtering\"](https://arxiv.org/abs/1708.05031) by He Xiangnan, Liao Lizi, Zhang Hanwang, Nie Liqiang, Hu Xia and Chua Tat-Seng - In Porc. of the 26th Interantional Conference on World Wide Web - 2017."
      ],
      "metadata": {
        "id": "ZxysEMZCO5sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Recommender System\n",
        "\n",
        "Recommender systems are algorithms that mimic the psychology and personality of humans, in order to predict their needs and desires. More formally, recommender systems adopt data-mining and machine-learning techniques to help users in finding attractive and useful products. Products can be almost anything: physical items (e.g., smartphones), places (e.g., restaurants), digital content (e.g., movies and music), and many more. Recommender systems produce recommendations based on different inputs: demographic information about users, ratings and comments on products, individual’s or community’s past preferences and choices, social networks, context of use.\n",
        "\n",
        "There are many different types of techniques and implementations out there.\n",
        "\n",
        "- **Content-based methods** uses attributes of items to recommend to users new items similar to what the user has liked in the past (doesn't take into account the behaviour of other users);\n",
        "- **Collaborative Filtering methods** uses similarities between users and items simultaneously to determine recommendations;\n",
        "- **Hybrid methos** mix content-based and collaborative filtering approaches.\n",
        "\n",
        "Other approaches are also called **Knowlege-based methods** which uses explicit knowledge about users and items to build recommendations criteria with a rule-based approach.\n",
        "\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1Qaizz9YLvqgXg0blWFwN92IQSQPLTSoH\" width=\"950\" height=\"400\"> </center>\n",
        "\n",
        "\n",
        "In the following we'll focus on Collaborative Filtering methods, with a model-based approach with deep learning algorithms.\n",
        "\n",
        "### Problem Definition\n",
        "\n",
        "Given a past record of movies seen by a user, we will build a recommender system that helps the user discover movies of their interest.\n",
        "\n",
        "Specifically, given <userID, itemID> occurrence pairs, we need to generate a ranked list of movies for each user.\n",
        "\n",
        "We model the problem as a binary classification problem, where we learn a function to predict whether a particular user will like a particular movie or not.\n",
        "\n",
        "$$f(userid, itemid) →, [0,1]$$\n",
        "\n",
        "The model takes in two sparse vectors, one representing the user and the other represents items. The users vector has size #users, while the items vector has size #items.  \n",
        "\n",
        "So, elaborately,\n",
        "- User vector=[0,0,1...,0,0,0] with m elements, means this vector represents the 3 rd user out of m.\n",
        "- Item vector=[0,1,0,0,0,0...0] with n elements, means this vector represents the 2 nd item out of n.\n",
        "\n",
        "Basically both items and users are one-hot encoded.\n",
        "\n",
        "These two vectors should be passed to a first embedding layer  (to project sparse representations to dense ones). These embeddings can be seen as a latent vector for users and items.\n",
        "\n",
        "Thus, the final predictive model will be\n",
        "$$y_{ui} = f(\\mathbf{P}^T\\mathbf{v}_u^T, \\mathbf{Q}^T\\mathbf{v}_i^T | \\mathbf{P}, \\mathbf{Q}, \\mathbf{\\Theta}_f) $$\n",
        "\n",
        "where **P** and **Q** denotes the latent factor matrix for users and items and **$\\Theta_f$** denotes the model parameters.\n"
      ],
      "metadata": {
        "id": "vwK9BJ1bss0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages"
      ],
      "metadata": {
        "id": "9131NU_plgbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yji9-7jslKRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arJu3Re7XCky",
        "outputId": "8f81349f-7dcf-44ef-e900-809c072b98d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79ecc1462a10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "IkcVzJcY5TS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwfzbOkACKsp",
        "outputId": "cb137211-3cee-422e-fada-2ea43d0301de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Movielens Dataset\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We use the MovieLens 100K dataset, which has 100,000 ratings from 1000 users on 1700 movies.\n",
        "\n",
        "The ratings are given to us in form of <userID,itemID, rating, timestamp> tuples. Each user has a minimum of 20 ratings.\n",
        "\n",
        "You can download the dataset [here](https://grouplens.org/datasets/movielens/). Download the file ml-100k.zip. Unzip it and extract the file u.data\n",
        "\n",
        "### Create Dataset\n",
        "\n",
        "Here we are going to create the necessary dataset for building recommender system.\n",
        "\n",
        "After downloading the file as indicated, save them in your Colab Notebooks directory. In order to let the Notebook see the file on your Drive you have to mount it."
      ],
      "metadata": {
        "id": "epmWgmA5w920"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_origin = {'100k': 'u.data', '1M': 'ratings.dat'}\n",
        "\n",
        "num_sample_data = '100k'\n",
        "DATA_PATH = '/content/drive/MyDrive/Advanced ML DL/{}'.format(dataset_origin[num_sample_data]) #change this with your directory\n",
        "MODEL_PATH = '/content/drive/MyDrive/Advanced ML DL/movielens_{}/'.format(num_sample_data) #change this with your directory\n"
      ],
      "metadata": {
        "id": "qcffYFMdxDcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now build some function to manage the data.\n",
        "\n",
        "We drop the exact value of rating (1,2,3,4,5) and instead convert it to an implicit scenario i.e. any positive interaction is given value of 1. All other interactions are given a value of zero, by default.\n",
        "\n",
        "Since we are training a classifier, we need both positive and negative samples. The records present in the dataset are counted as positive samples. We assume that all entries in the user-item interaction matrix are negative samples (a strong assumption, and easy to implement).\n",
        "\n",
        "We randomly sample 4 items that are not interacted by the user, for every item interacted by the user. This way, if a user has 20 positive interactions, he will have 80 negative interactions. These negative interactions cannot contain any positive interaction by the user, though they may not be all unique due to random sampling."
      ],
      "metadata": {
        "id": "kZ43K6UT9UmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the class MovieLens Dataset which will be used do read the data and create the train and test dataset."
      ],
      "metadata": {
        "id": "tE13kg0aydtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rating_Datset(Dataset):\n",
        "\tdef __init__(self, user_list, item_list, rating_list):\n",
        "\t\tsuper(Rating_Datset, self).__init__()\n",
        "\t\tself.user_list = user_list\n",
        "\t\tself.item_list = item_list\n",
        "\t\tself.rating_list = rating_list\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.user_list)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tuser = self.user_list[idx]\n",
        "\t\titem = self.item_list[idx]\n",
        "\t\trating = self.rating_list[idx]\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.tensor(user, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(item, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(rating, dtype=torch.float)\n",
        "\t\t\t)"
      ],
      "metadata": {
        "id": "hmlijxEoYXDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NCF_Data(object):\n",
        "\t\"\"\"\n",
        "\tConstruct Dataset for NCF\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, args, ratings):\n",
        "\t\tself.ratings = ratings\n",
        "\t\tself.num_ng = args.num_ng\n",
        "\t\tself.num_ng_test = args.num_ng_test\n",
        "\t\tself.batch_size = args.batch_size\n",
        "\n",
        "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
        "\n",
        "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
        "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
        "\n",
        "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
        "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
        "\n",
        "\n",
        "\tdef _reindex(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
        "\t\t\"\"\"\n",
        "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
        "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
        "\n",
        "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
        "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
        "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
        "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
        "\t\treturn ratings\n",
        "\n",
        "\tdef _leave_one_out(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
        "\t\t\"\"\"\n",
        "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
        "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
        "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
        "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
        "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
        "\n",
        "\tdef _negative_sampling(self, ratings):\n",
        "\t\tinteract_status = (\n",
        "\t\t\tratings.groupby('user_id')['item_id']\n",
        "\t\t\t.apply(set)\n",
        "\t\t\t.reset_index()\n",
        "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
        "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
        "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
        "\n",
        "\tdef get_train_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
        "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
        "\t\tfor row in train_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in range(self.num_ng):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(row.negatives[i]))\n",
        "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\tdef get_test_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
        "\t\tfor row in test_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(i))\n",
        "\t\t\t\tratings.append(float(0))\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "FxZCDy4tYkRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "We randomly sample 100 items that are not interacted by the user, ranking the test item among the 100 items. We truncate the ranked list at 10.\n",
        "\n",
        "Since it is too time-consuming to rank all items for every user, for we will have to calculate 1000\\*1700 ~10⁶ values. With this strategy, we need 1000*100 ~ 10⁵ values, an order of magnitude less.\n",
        "\n",
        "For each user, we use the latest rating(according to timestamp) in the test set, and we use the rest for training. This evaluation methodology is also known as leave-one-out strategy.\n",
        "\n",
        "#### Metrics\n",
        "\n",
        "We use **Hit Ratio** (HR), and **Normalized Discounted Cumulative Gain** (NDCG) to evaluate the performance for our RS.\n",
        "\n",
        "\n",
        "In recommender settings, the **HR** is simply the fraction of users for which the correct answer is included in the recommendation list of length L.\n",
        "\n",
        "$$HR = \\frac{|U_{hit}^{L}|}{U_{all}}$$\n",
        "\n",
        "Where $U_{hit}^{L}$ is the number of users for which the correct answer is included in the top L recommendation list, while, $U_{all}$ is the total number of user in the test dataset. Clearly the larger L is the larger HR become.\n",
        "\n",
        "**NDCG** is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks\n",
        "\n",
        "**Gain** for an item is essentialy the same as the relevance score, which can be numerical ratings like search results in Google which can be rated in scale from 1 to 5, or binary in case of implicit data where we only know if a user has consumed certain item or not. Naturally **Cumulative Gain** is defined as the sum of gains up to a position k in the recommendation list.\n",
        "\n",
        "$$CG(k) = \\sum_{i=1}^{k}G_i$$\n",
        "\n",
        "One obvious drawback of CG is that it does not take into account of ordering. By swapping the relative order of any two items, the CG would be unaffected. This is problematic when ranking order is important. For example, on Google Search results, you would obviously not like placing the most relevant web page at the bottom.\n",
        "\n",
        "To penalize highly relevant items being placed at the bottom, we introduce the **Discounted Cumulative Gaing** (DCG).\n",
        "\n",
        "$$DCG(k) = \\sum_{i=1}^{k} \\frac{G_i}{log_2(i+1)}$$\n",
        "\n",
        "By diving the gain by its rank, we sort of push the algorithm to place highly relevant items to the top to achieve the best DCG score.\n",
        "\n",
        "There is still a drawback of DCG score. It is that DCG score adds up with the length of recommendation list. Therefore, we cannot consistently compare the DCG score for system recommending top 5 and top 10 items, because the latter will have higher score not because its recommendation quality but pure length.\n",
        "\n",
        "We tackle this issue by introducing **Ideal Discounted Cumulative Gain** (IDCG). IDCG is the DCG score for the most ideal ranking, which is ranking the items top down according their relevance up to position k.\n",
        "\n",
        "$$IDCG(k) = \\sum_{i=1}^{|I(k)|} \\frac{G_i}{log_2(i+1)}$$\n",
        "\n",
        "Where $|I(k)|$ represent the ideal list of items up to position k.\n",
        "\n",
        "And NDCG is simply to normalize the DCG score by IDCG such that its value is always between 0 and 1 regardless of the length.\n",
        "\n",
        "$$NDCD(k) = \\frac{DCG(k)}{IDCG(k)}$$\n",
        "\n",
        "Our model gives a confidence score between 0 and 1 for each item present in the test set for a given user. The items are sorted in decreasing order of their score, and top 10 items are given as recommendation. If the test item (which is only one for each user) is present in this list, HR is one for this user, else it is zero. The final HR is reported after averaging for all users. A similar calculation is done for NDCG.\n",
        "\n",
        "While training, we will be minimizing the cross-entropy loss, which is the standard loss function for a classification problem. The real strength of RS lies in giving a ranked list of top-k items, which a user is most likely to interact. Think about why you mostly click on google search results only on the first page, and never go to other pages. Metrics like NDCG and HR help in capturing this phenomenon by indicating the quality of our ranked lists."
      ],
      "metadata": {
        "id": "qx2UpNblDKH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hit(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\tindex = pred_items.index(ng_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k, device):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
        "\t\tHR.append(hit(ng_item, recommends))\n",
        "\t\tNDCG.append(ndcg(ng_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ],
      "metadata": {
        "id": "uTaIX7H52i-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Matrix Factorization (GMF)\n",
        "\n",
        "Generally Matrix Factorization (MF) algorithms associates each user and item with a real-valued vector of latent features.\n",
        "\n",
        "Let $\\mathbf{p_u}$ and $\\mathbf{q_i}$ denote the latent vector for user u and item i, respectively; MF estimates an interaction y_{ui} as the inner product of $\\mathbf{p_u}$ and $\\mathbf{q_i}$:\n",
        "\n",
        "$$y_{ui} = f(u, i| \\mathbf{p_u}, \\mathbf{q_i}) = \\mathbf{p_u}^T\\mathbf{q_i} = \\sum_{k=1}^Kp_{uk}q_{ik} $$\n",
        "\n",
        "Where K denotes the dimension of the latent space.\n",
        "\n",
        "MF models the dimension of the interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them  with the same weights.\n",
        "\n",
        "This imposes some limitation of MF caused by the use of a simple and fixed inner product to estimate complex user-item interactions in the low-dimensional latent space.\n",
        "\n",
        "In order to overcome this limitation, we can build a Generalized Matrix Factorization (GMF) algorithm where we can weight the linear combination of the element-wise product and use this with an activation function to learn a representation of the input insted of using a fixed one.\n",
        "\n",
        "Let the user latent vector $\\mathbf{p_u}$  be denoted as $\\mathbf{P}^T\\mathbf{v}_u^T$ and the item latent vector $\\mathbf{q_i}$ as $\\mathbf{Q}^T\\mathbf{v}_i^T$.\n",
        "\n",
        "The GMF can be expressed as\n",
        "$$Y_{ui} = a_{out}(\\mathbf{h}^T(\\mathbf{p_u}\\odot \\mathbf{q_i})$$\n",
        "where $a_{out}$ is the activation function and $\\mathbf{h}$ are the edge weights of the output layer.\n",
        "\n",
        "Intuitively, if we use an identity function as $a_{out}$ and enforce $\\mathbf{h}$ to be a uniform vector of 1, we can exactly recover the MF model.\n",
        "\n",
        "In the following we will use $\\mathbf{h}$ as linear layer and a $a_{out}$ as sigmoid function to learn $\\mathbf{h}$ weigths from data with the log loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "KuigI6hAj6Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GMF(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "DTMHganmd5q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Collaborative Filtering\n",
        "GMF learn user and item embedding separately. To empower the model it is possible to combine the features of two pathways by concatenating them and passing this concatenation to a Multi-Layer Perceptron (MLP) to learn the interaction between user and item latent features. This model is known as Neural Collaborative Filtering (NCF)\n",
        "\n",
        "The input to the model is userID and itemID, which is fed into an embedding layer. Thus, each user and item is given an embedding. Then there are multiple dense layers afterward, followed by a single neuron with a sigmoid activation.\n",
        "\n",
        "The output of the sigmoid neuron can be interpreted as the probability the user is likely to interact with an item.\n",
        "\n",
        "The model, we are going to implement is the following:\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1rL_8kkHIhSlQjWr8hNal4Tyog87-2kNP\" width=\"550\" height=\"350\"> </center>\n",
        "\n",
        "The user in item vectors are passed to an embedding layer that build a dense or latent vectors for the sparse inputs, from the input layer. The obtained latent vectors are fed into the multi-layer neural architecture, to map the latent vectors to the predicted probability scores. The layers are responsible to find the complex user-item relations from the data.\n",
        "\n",
        "The output layer produces the predicted score $y_(ui)$, i.e, how much is the probability that the user u will interact with the item i.\n",
        "\n",
        "A pointwise loss function is used to minimize the difference between the target value Y(ui) and the corresponding predicted value.\n",
        "\n",
        "Formally the model we are going to implement is the following:\n",
        "\n",
        "$$\\mathbf{z_1} = \\phi_1 (\\mathbf{p_u}, \\mathbf{q_i}) = \\begin{bmatrix}\n",
        "\\mathbf{p_u} \\\\ \\mathbf{q_i}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$\\phi_L(\\mathbf{z}_{L-1}) = a_L(\\mathbf{W}_L^T\\mathbf{z}_{L-1} + \\mathbf{b}_L)$$\n",
        "\n",
        "$$ y_{ui} = \\sigma(\\mathbf{h}^T\\phi_L(\\mathbf{z}_{L-1}))$$\n",
        "\n",
        "where $\\mathbf{W}_x, \\mathbf{b}_x, a_x$ denotes the weight matrix, bias vector and activation function for the x-th layer's perceptron\n"
      ],
      "metadata": {
        "id": "FSUXUsUS5OoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(MLP, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "        self.layers = args.layers\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
        "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = nn.ReLU()(vector)\n",
        "            # vector = nn.BatchNorm1d()(vector)\n",
        "            # vector = nn.Dropout(p=0.5)(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "sokr3I_8d5tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse-hide\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\",\n",
        "\ttype=int,\n",
        "\tdefault=42,\n",
        "\thelp=\"Seed\")\n",
        "parser.add_argument(\"--lr\",\n",
        "\ttype=float,\n",
        "\tdefault=0.001,\n",
        "\thelp=\"learning rate\")\n",
        "parser.add_argument(\"--dropout\",\n",
        "\ttype=float,\n",
        "\tdefault=0.2,\n",
        "\thelp=\"dropout rate\")\n",
        "parser.add_argument(\"--batch_size\",\n",
        "\ttype=int,\n",
        "\tdefault=256,\n",
        "\thelp=\"batch size for training\")\n",
        "parser.add_argument(\"--epochs\",\n",
        "\ttype=int,\n",
        "\tdefault=30,\n",
        "\thelp=\"training epoches\")\n",
        "parser.add_argument(\"--top_k\",\n",
        "\ttype=int,\n",
        "\tdefault=10,\n",
        "\thelp=\"compute metrics@top_k\")\n",
        "parser.add_argument(\"--factor_num\",\n",
        "\ttype=int,\n",
        "\tdefault=32,\n",
        "\thelp=\"predictive factors numbers in the model\")\n",
        "parser.add_argument(\"--layers\",\n",
        "    nargs='+',\n",
        "    default=[64,32,16,8],\n",
        "    help=\"MLP layers. Note that the first layer is the concatenation of user \\\n",
        "    and item embeddings. So layers[0]/2 is the embedding size.\")\n",
        "parser.add_argument(\"--num_ng\",\n",
        "\ttype=int,\n",
        "\tdefault=4,\n",
        "\thelp=\"Number of negative samples for training set\")\n",
        "parser.add_argument(\"--num_ng_test\",\n",
        "\ttype=int,\n",
        "\tdefault=100,\n",
        "\thelp=\"Number of negative samples for test set\")\n",
        "parser.add_argument(\"--out\",\n",
        "\tdefault=True,\n",
        "\thelp=\"save model or not\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzFNcyild5y8",
        "outputId": "3f48c4ae-f158-41d2-ad93-5d11099d452d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--out'], dest='out', nargs=None, const=None, default=True, type=None, choices=None, required=False, help='save model or not', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH,\n",
        "\tsep=\"\\t\",\n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'],\n",
        "\tengine='python')\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()\n",
        "\n",
        "# set model and loss, optimizer\n",
        "model = GMF(args, num_users, num_items)\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# train, evaluation\n",
        "best_hr = 0\n",
        "for epoch in range(1, args.epochs+1):\n",
        "\tmodel.train() # Enable dropout (if have).\n",
        "\tstart_time = time.time()\n",
        "\n",
        "\tfor user, item, label in train_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\t\tlabel = label.to(device)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tprediction = model(user, item)\n",
        "\t\tloss = loss_function(prediction, label)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
        "\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" +\n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\tif args.out:\n",
        "\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\ttorch.save(model,\n",
        "\t\t\t\t'{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coXQgQWYd51r",
        "outputId": "ed65225b-c98e-400b-fd67-78e5c3b3a92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-be6c71ee6579>:52: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
            "<ipython-input-15-be6c71ee6579>:58: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 time to train: 00: 00: 31\n",
            "HR: 0.109\tNDCG: 0.050\n",
            "Epoch 002 time to train: 00: 00: 37\n",
            "HR: 0.107\tNDCG: 0.051\n",
            "Epoch 003 time to train: 00: 00: 29\n",
            "HR: 0.128\tNDCG: 0.057\n",
            "Epoch 004 time to train: 00: 00: 29\n",
            "HR: 0.180\tNDCG: 0.086\n",
            "Epoch 005 time to train: 00: 00: 29\n",
            "HR: 0.263\tNDCG: 0.127\n",
            "Epoch 006 time to train: 00: 00: 29\n",
            "HR: 0.313\tNDCG: 0.158\n",
            "Epoch 007 time to train: 00: 00: 29\n",
            "HR: 0.347\tNDCG: 0.178\n",
            "Epoch 008 time to train: 00: 00: 30\n",
            "HR: 0.369\tNDCG: 0.189\n",
            "Epoch 009 time to train: 00: 00: 34\n",
            "HR: 0.375\tNDCG: 0.196\n",
            "Epoch 010 time to train: 00: 00: 31\n",
            "HR: 0.396\tNDCG: 0.211\n",
            "Epoch 011 time to train: 00: 00: 31\n",
            "HR: 0.425\tNDCG: 0.223\n",
            "Epoch 012 time to train: 00: 00: 30\n",
            "HR: 0.443\tNDCG: 0.234\n",
            "Epoch 013 time to train: 00: 00: 31\n",
            "HR: 0.454\tNDCG: 0.240\n",
            "Epoch 014 time to train: 00: 00: 29\n",
            "HR: 0.467\tNDCG: 0.248\n",
            "Epoch 015 time to train: 00: 00: 30\n",
            "HR: 0.487\tNDCG: 0.256\n",
            "Epoch 016 time to train: 00: 00: 29\n",
            "HR: 0.491\tNDCG: 0.261\n",
            "Epoch 017 time to train: 00: 00: 30\n",
            "HR: 0.496\tNDCG: 0.270\n",
            "Epoch 018 time to train: 00: 00: 29\n",
            "HR: 0.503\tNDCG: 0.275\n",
            "Epoch 019 time to train: 00: 00: 31\n",
            "HR: 0.509\tNDCG: 0.278\n",
            "Epoch 020 time to train: 00: 00: 31\n",
            "HR: 0.512\tNDCG: 0.280\n",
            "Epoch 021 time to train: 00: 00: 32\n",
            "HR: 0.522\tNDCG: 0.283\n",
            "Epoch 022 time to train: 00: 00: 34\n",
            "HR: 0.523\tNDCG: 0.285\n",
            "Epoch 023 time to train: 00: 00: 30\n",
            "HR: 0.530\tNDCG: 0.289\n",
            "Epoch 024 time to train: 00: 00: 30\n",
            "HR: 0.532\tNDCG: 0.290\n",
            "Epoch 025 time to train: 00: 00: 30\n",
            "HR: 0.525\tNDCG: 0.291\n",
            "Epoch 026 time to train: 00: 00: 29\n",
            "HR: 0.527\tNDCG: 0.295\n",
            "Epoch 027 time to train: 00: 00: 29\n",
            "HR: 0.530\tNDCG: 0.297\n",
            "Epoch 028 time to train: 00: 00: 31\n",
            "HR: 0.531\tNDCG: 0.299\n",
            "Epoch 029 time to train: 00: 00: 31\n",
            "HR: 0.537\tNDCG: 0.302\n",
            "Epoch 030 time to train: 00: 00: 29\n",
            "HR: 0.543\tNDCG: 0.306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqrWhSirBV8C",
        "outputId": "d402f053-05e7-4de2-8e6b-5f8de726270e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GMF(\n",
              "  (embedding_user): Embedding(944, 32)\n",
              "  (embedding_item): Embedding(1683, 32)\n",
              "  (affine_output): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (logistic): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH,\n",
        "\tsep=\"\\t\",\n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'],\n",
        "\tengine='python')"
      ],
      "metadata": {
        "id": "IqBqcH7l4rwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_100k.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "LQs3jDWfFFKR",
        "outputId": "c69d839d-1598-43b2-a1b7-f3ace41757c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            user_id        item_id         rating     timestamp\n",
              "count  100000.00000  100000.000000  100000.000000  1.000000e+05\n",
              "mean      462.48475     425.530130       3.529860  8.835289e+08\n",
              "std       266.61442     330.798356       1.125674  5.343856e+06\n",
              "min         1.00000       1.000000       1.000000  8.747247e+08\n",
              "25%       254.00000     175.000000       3.000000  8.794487e+08\n",
              "50%       447.00000     322.000000       4.000000  8.828269e+08\n",
              "75%       682.00000     631.000000       4.000000  8.882600e+08\n",
              "max       943.00000    1682.000000       5.000000  8.932866e+08"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3289ff45-0b7c-476b-88e0-383860623ffb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100000.00000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>1.000000e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>462.48475</td>\n",
              "      <td>425.530130</td>\n",
              "      <td>3.529860</td>\n",
              "      <td>8.835289e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>266.61442</td>\n",
              "      <td>330.798356</td>\n",
              "      <td>1.125674</td>\n",
              "      <td>5.343856e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.747247e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>254.00000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.794487e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>447.00000</td>\n",
              "      <td>322.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.828269e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>682.00000</td>\n",
              "      <td>631.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.882600e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>943.00000</td>\n",
              "      <td>1682.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>8.932866e+08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3289ff45-0b7c-476b-88e0-383860623ffb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3289ff45-0b7c-476b-88e0-383860623ffb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3289ff45-0b7c-476b-88e0-383860623ffb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-04ee5012-81fb-42df-8147-432f59fd21d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04ee5012-81fb-42df-8147-432f59fd21d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-04ee5012-81fb-42df-8147-432f59fd21d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"ml_100k\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35202.13818053956,\n        \"min\": 1.0,\n        \"max\": 100000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          462.48475,\n          447.0,\n          100000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"item_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35178.88603210694,\n        \"min\": 1.0,\n        \"max\": 100000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          425.53013,\n          322.0,\n          100000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35354.245317453555,\n        \"min\": 1.0,\n        \"max\": 100000.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          100000.0,\n          3.52986,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 407843582.9913888,\n        \"min\": 100000.0,\n        \"max\": 893286638.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          883528851.48862,\n          882826944.0,\n          100000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH,\n",
        "\tsep=\"\\t\",\n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'],\n",
        "\tengine='python')\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()\n",
        "\n",
        "# set model and loss, optimizer\n",
        "model = MLP(args, num_users, num_items)\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# train, evaluation\n",
        "best_hr = 0\n",
        "for epoch in range(1, args.epochs+1):\n",
        "\tmodel.train() # Enable dropout (if have).\n",
        "\tstart_time = time.time()\n",
        "\n",
        "\tfor user, item, label in train_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\t\tlabel = label.to(device)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tprediction = model(user, item)\n",
        "\t\tloss = loss_function(prediction, label)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
        "\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" +\n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\tif args.out:\n",
        "\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\ttorch.save(model,\n",
        "\t\t\t\t'{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npAaN3Cct7Sa",
        "outputId": "6006d201-07f6-4a76-a4ba-3d35b277b9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-be6c71ee6579>:52: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
            "<ipython-input-15-be6c71ee6579>:58: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 time to train: 00: 00: 37\n",
            "HR: 0.405\tNDCG: 0.213\n",
            "Epoch 002 time to train: 00: 00: 32\n",
            "HR: 0.402\tNDCG: 0.223\n",
            "Epoch 003 time to train: 00: 00: 34\n",
            "HR: 0.394\tNDCG: 0.222\n",
            "Epoch 004 time to train: 00: 00: 33\n",
            "HR: 0.406\tNDCG: 0.226\n",
            "Epoch 005 time to train: 00: 00: 34\n",
            "HR: 0.409\tNDCG: 0.227\n",
            "Epoch 006 time to train: 00: 00: 34\n",
            "HR: 0.449\tNDCG: 0.243\n",
            "Epoch 007 time to train: 00: 00: 33\n",
            "HR: 0.472\tNDCG: 0.262\n",
            "Epoch 008 time to train: 00: 00: 37\n",
            "HR: 0.485\tNDCG: 0.271\n",
            "Epoch 009 time to train: 00: 00: 35\n",
            "HR: 0.508\tNDCG: 0.282\n",
            "Epoch 010 time to train: 00: 00: 35\n",
            "HR: 0.515\tNDCG: 0.293\n",
            "Epoch 011 time to train: 00: 00: 33\n",
            "HR: 0.523\tNDCG: 0.298\n",
            "Epoch 012 time to train: 00: 00: 35\n",
            "HR: 0.522\tNDCG: 0.299\n",
            "Epoch 013 time to train: 00: 00: 34\n",
            "HR: 0.537\tNDCG: 0.304\n",
            "Epoch 014 time to train: 00: 00: 35\n",
            "HR: 0.539\tNDCG: 0.303\n",
            "Epoch 015 time to train: 00: 00: 33\n",
            "HR: 0.544\tNDCG: 0.316\n",
            "Epoch 016 time to train: 00: 00: 36\n",
            "HR: 0.541\tNDCG: 0.313\n",
            "Epoch 017 time to train: 00: 00: 34\n",
            "HR: 0.554\tNDCG: 0.320\n",
            "Epoch 018 time to train: 00: 00: 33\n",
            "HR: 0.544\tNDCG: 0.315\n",
            "Epoch 019 time to train: 00: 00: 33\n",
            "HR: 0.555\tNDCG: 0.316\n",
            "Epoch 020 time to train: 00: 00: 33\n",
            "HR: 0.541\tNDCG: 0.312\n",
            "Epoch 021 time to train: 00: 00: 34\n",
            "HR: 0.556\tNDCG: 0.313\n",
            "Epoch 022 time to train: 00: 00: 32\n",
            "HR: 0.559\tNDCG: 0.313\n",
            "Epoch 023 time to train: 00: 00: 33\n",
            "HR: 0.559\tNDCG: 0.314\n",
            "Epoch 024 time to train: 00: 00: 37\n",
            "HR: 0.548\tNDCG: 0.313\n",
            "Epoch 025 time to train: 00: 00: 40\n",
            "HR: 0.557\tNDCG: 0.311\n",
            "Epoch 026 time to train: 00: 00: 34\n",
            "HR: 0.546\tNDCG: 0.308\n",
            "Epoch 027 time to train: 00: 00: 33\n",
            "HR: 0.561\tNDCG: 0.309\n",
            "Epoch 028 time to train: 00: 00: 34\n",
            "HR: 0.563\tNDCG: 0.313\n",
            "Epoch 029 time to train: 00: 00: 33\n",
            "HR: 0.567\tNDCG: 0.318\n",
            "Epoch 030 time to train: 00: 00: 34\n",
            "HR: 0.569\tNDCG: 0.314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oImBUunw-uzo",
        "outputId": "d63307a7-dc70-4c2e-a325-bc4fe5baed84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (embedding_user): Embedding(944, 32)\n",
              "  (embedding_item): Embedding(1683, 32)\n",
              "  (fc_layers): ModuleList(\n",
              "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
              "  )\n",
              "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
              "  (logistic): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAB CHALLENGE: Neural Matrix Factorization\n",
        "So far we have developed GMF that applies a linear kernel to model the latent feature interactions and MLP that uses a non-linear kernel to learn the interaction function from data.\n",
        "\n",
        "How can we fuse GMF and MLP under the NCF framework, so that they can mutually reinforce each other to better model the complex user-item interactions?\n",
        "\n",
        "A straightforward solution is to let GMF and MLP share the same embedding layer and then combine the outputs of their interaction functions.\n",
        "\n",
        "To provide flexibility to the fused model we want to allow GMF and MLP to learn separate embeddings and combine two models by concatenating their last hidden layer.\n",
        "\n",
        "Formally\n",
        "\n",
        "$$\\phi^{GMF} = \\mathbf{p_u}^G\\odot \\mathbf{q_i}^G$$\n",
        "$$\\phi^{MLP} = a_L(\\mathbf{W}_L^T(a_{L-1}(...a_2 (\\mathbf{W}_2^T \\begin{bmatrix}\n",
        "\\mathbf{p_u} \\\\ \\mathbf{q_i}\n",
        "\\end{bmatrix} + \\mathbf{b}_2)...)) + \\mathbf{b}_L)$$\n",
        "\n",
        "$$ y_{ui} = \\sigma(\\mathbf{h}^T \\begin{bmatrix}\n",
        "\\ \\phi^{GMF} \\\\ \\phi^{MLP}\n",
        "\\end{bmatrix})$$\n",
        "\n",
        "that results in the following model architecture\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1gNLUpiQdbDPMdvfZYVs3lcou3cd4Favb\" width=\"550\" height=\"400\"> </center>\n",
        "\n",
        "\n",
        "- TASK 1: implement the model as described\n",
        "- TASK 2: compare the performance of such model with the GMS and MLP models using the metrics provided.\n",
        "- TASK 3: tune the networks by plotting HR@10 and NDCG@10 with respect to the number of predictive factors [8, 16, 32, 64] for all the 3 algorithms\n"
      ],
      "metadata": {
        "id": "nGl8O6kYqnvq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCebWgM1d54J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFWYG-w7d56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-WtIEPjOd588"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73RGBr1wd5_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}